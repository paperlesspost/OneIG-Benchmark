# OneIG-Bench
## OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation

<div align="center">
 <a href="https://oneig-bench.github.io/"><img src="https://img.shields.io/static/v1?label=Project%20Page&message=Web&color=green"></a> &ensp;
 <a href="https://huggingface.co/datasets/OneIG-Bench/OneIG-Bench"><img src="https://img.shields.io/static/v1?label=Dataset&message=Huggingface&color=yellow"></a> &ensp;
  <a href="https://arxiv.org/abs/xxxx.xxxxx"><img src="https://img.shields.io/static/v1?label=Tech%20Report&message=Arxiv&color=red"></a> &ensp;
</div>

<br>  
<br>  

<p align="center">
    <img src="./assets/OneIG-Bench.png" width="92%">
</p>

## ğŸ”¥ğŸ”¥ğŸ”¥ News

* **`2025.06.10`** ğŸŒŸ We released the [**OneIG-Bench**](https://huggingface.co/datasets/OneIG-Bench/OneIG-Bench) benchmark on ğŸ¤—huggingface.
* **`2025.06.10`** ğŸŒŸ We released the [tech report](https://arxiv.org/abs/xxxx.xxxxx) and the [project page](https://oneig-bench.github.io/)
* **`2025.06.10`** ğŸŒŸ We released the evaluation scripts. 

## To Do List
- [ ] Real-time Updating Leaderboard
- [x] OneIG-Bench Release 
- [x] Evaluation Scripts, Technical Report & Project Page Release

## Introduction

We introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including subject-element alignment, text rendering precision, reasoning-generated content, stylization, and diversity. Specifically, these dimensions can be flexibly selected for evaluation based on specific needs.

Key contribution:

- We present **OneIG-Bench**, which consists of six prompt sets, with the first five â€” 245 *Anime and Stylization*, 244 *Portrait*, 206 *General Object*, 200 *Text Rendering*, and 225 *Knowledge and Reasoning* prompts â€” each provided in both English and Chinese, and 200 *Multilingualism* prompts, designed for the comprehensive evaluation of current text-to-image models.
- A systematic quantitative evaluation is developed to facilitate objective capability ranking through standardized metrics, enabling direct comparability across models. Specifically, our evaluation framework allows T2I models to generate images only for prompts associated with a particular evaluation dimension, and to assess performance accordingly within that dimension.
- State-of-the-art open-sourced methods as well as the proprietary model are evaluated based on our proposed benchmark to facilitate the development of text-to-image research.

## Get Started

### Dependencies and Installation:
We test our benchmark using torch==2.6.0, torchvision==0.21.0 with cuda-11.8, python==3.10.

Install requirements:
  
``` bash
pip install -r requirements.txt
```

The version of flash-attention is in the last line of [`requirements.txt`](requirements.txt). Download the [CLIP model](https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt) and [CSD model](https://drive.google.com/file/d/1FX0xs8p-C7Ob-h5Y4cUhTeOepHzXv_46/view?usp=sharing) and save them under `./scripts/style/models`.


### Image Generation
It's better for you to generate 4 images for each prompt in OneIG-Bench. And combine these 4 images into a single image. Each prompt's generated images should be saved into subfolders based on their category **Anime & Stylization, Portrait, General Object, Text Rendering, Knowleddge & Reasoning, Multilingualism**, corresponding to folders **anime, human, object, text, reasoning, multilingualism**. If any image cannot be generated, I suggest saving a black image with the specified filename.

The filename for each image should follow the id assigned to that prompt in [`OneIG-Bench.csv`](OneIG-Bench.csv)/[`OneIG-Bench-ZH.csv`](OneIG-Bench-ZH.csv). The structure of the images to be saved should look like:

```bash
ğŸ“ images/
â”œâ”€â”€ ğŸ“‚ anime/                  
â”‚   â”œâ”€â”€ ğŸ“‚ gpt-4o/
â”‚   â”‚   â”œâ”€â”€ 000.webp
â”‚   â”‚   â”œâ”€â”€ 001.webp
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ğŸ“‚ imagen4/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ human/               
â”‚   â”œâ”€â”€ ğŸ“‚ gpt-4o/
â”‚   â”œâ”€â”€ ğŸ“‚ imagen4/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ object/                
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ text/                  
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ reasoning/             
â”‚   â””â”€â”€ ...
â””â”€â”€ ğŸ“‚ multilingualism/        # For OneIG-Bench-ZH
    â””â”€â”€ ...
```

## Evaluation

### Scripts
```shell
./run_{overall, alignment, diversity, reasoning, style, text}.sh
```
The [`run_overall.sh`](run_overall.sh) script contains the execution of all metrics. By running `run_overall.sh`, you can obtain the results of all metrics in the results directory. You can also choose the metric you want to evaluate by running the corresponding script: `run_{metric_name}.sh`.

### Parameters Configuration for Evaluation

To ensure that the generated images are correctly loaded for evaluation, you can modify the following parameters in each script:

1. **`mode`** : Select whether **EN/ZH** to evaluate on **OneIG-Bench** or **OneIG-Bench-ZH**.  

2. **`image_dir`** : The directory where the images generated by your model are stored.  

3. **`model_names`** : The names or identifiers of the models you want to evaluate.  

4. **`image_grid`** : This corresponds to the number of images generated by the model per prompt, where a value of **1** means 1 image, **2** means 4 images, and so on.

5. **`class_items`** : The prompt categories or image sets you want to evaluate.  

## ğŸ“ˆ Results

We define the sets of images generated based on the OneIG-Bench prompt categories: *General Object* (**O**), *Portrait* (**P**), *Anime and Stylization* (**A**) for prompts without stylization, (**S**) for prompts with stylization, *Text Rendering* (**T**), *Knowledge and Reasoning* (**KR**), and *Multilingualism* (**L**). 

The correspondence between the evaluation metrics and the evaluated image sets in `OneIG-Bench` and `OneIG-Bench-ZH` is presented in the table below.

- **ğŸ“Š Metrics and Image Sets Correspondence**

<div align="center">

|                    |                  Alignment                  |    Text    | Reasoning |   Style   |                  Diversity                 |
|--------------------|:-------------------------------------------:|:----------:|:---------:|:---------:|:------------------------------------------:|
| **OneIG-Bench**         | **O**, **P**, **A**, **S**                   | **T**      | **KR**    | **S**     | **O**, **P**, **A**, **S**, **T**, **KR**   |
| **OneIG-Bench-ZH**     | **O**<sub>zh</sub>, **P**<sub>zh</sub>, **A**<sub>zh</sub>, **S**<sub>zh</sub>, **L**<sub>zh</sub> | **T**<sub>zh</sub> | **KR**<sub>zh</sub> | **S**<sub>zh</sub> | **O**<sub>zh</sub>, **P**<sub>zh</sub>, **A**<sub>zh</sub>, **S**<sub>zh</sub>, **L**<sub>zh</sub>, **T**<sub>zh</sub>, **KR**<sub>zh</sub> |
</div>

- **Method Comparision on OneIG-Bench:**

<p align="center">
    <img src="./assets/result.png" width="96%">
</p>

- **Method Comparision on OneIG-Bench-ZH:**

<p align="center">
    <img src="./assets/result_ZH.png" width="96%">
</p>


- **Benchmark Comparison:**

&nbsp;&nbsp;&nbsp;&nbsp;**OneIG-Bench** (also referred to as **OneIG-Bench-EN**) denotes the English benchmark set.
<p align="center">
    <img src="./assets/benchmark_comparison.png" width="86%">
</p>


## Citation
If you find our work helpful for your research, please consider citing our work.

```bibtex
@article{chang2025oneig,
  title={OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation}, 
  author={Jingjing Chang and Yixiao Fang and Peng Xing and Shuhan Wu and Wei Cheng and Rui Wang and Xianfang Zeng and Gang Yu and Hai-Bao Chen},
  journal={arXiv preprint arxiv:xxxx.xxxxx},
  year={2025}
}
```

## Acknowledgement
We would like to express our sincere thanks to the contributors of [Qwen](https://github.com/QwenLM/Qwen2.5-VL),  [CLIP](https://github.com/openai/CLIP), [CSD_Score](https://github.com/haofanwang/CSD_Score), [DreamSim](https://github.com/ssundaram21/dreamsim), and [HuggingFace](https://huggingface.co) teams, for their open research and exploration.

